---
title: "Can Commonsense Knowledge Improve CLIP’s Performance in Cross-Domain VQA?"
collection: publications
category: conferences
permalink: 
excerpt: 
date: 2024-04-14
venue: 'ICVGIP 2024'
paperurl: 'https://dl.acm.org/doi/10.1145/3702250.3702265'
citation: 'Mohamad Hassan N C, Ankit Jha, Moloud Abdar, and Biplab Banerjee. 2025. Can Commonsense Knowledge Improve CLIP's Performance in Cross-Domain VQA?✱ In Proceedings of the Fifteenth Indian Conference on Computer Vision Graphics and Image Processing (ICVGIP '24). Association for Computing Machinery, New York, NY, USA, Article 15, 1–10. https://doi.org/10.1145/3702250.3702265'
---

In Visual Question Answering (VQA), the ability to effectively interpret images and questions across diverse domain distributions is imperative. Despite the critical need, there remains a significant gap in methodologies specifically tailored for Cross-Domain VQA (CD-VQA). Current methods predominantly retrofit standard VQA models with domain alignment mechanisms but often fail to address the subtle semantic and visual variances at both micro and macro scales. To overcome these challenges, we introduce a novel strategy, CS-CLIP, which leverages the generalizability of the pre-trained multi-modal foundation model, CLIP, integrated with commonsense knowledge. This integration aims to better bridge the visual and semantic nuances. We enhance the scene graph representations of images by amalgamating CLIP’s visual features with semantic contexts from a pre-trained knowledge graph, thereby producing improved node embeddings. We employ a trainable graph attention module to pinpoint crucial nodes within the scene graph. Additionally, we introduce a cross-attention module to assess the similarity between the question tokens and the scene-graph nodes, generating more directed feature embeddings for the answer decoder. Our comprehensive experiments across multiple benchmarks vividly demonstrate the efficacy of CS-CLIP, significantly enhancing the model’s ability to generalize across domains for VQA.
